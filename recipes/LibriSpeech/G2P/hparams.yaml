# Seed needs to be set at top of yaml, before objects with parameters are made
# NOTE: Seed does not guarantee replicability with CTC
seed: 1234
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]

# Data files
output_folder: !ref results/g2p/<seed>
data_folder: /home/lugosch/data/mini-librispeech #/home/mirco/datasets/lexicon
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

input_lexicon: !ref <data_folder>/lexicon.csv
oov: !ref <data_folder>/oov.csv
output_lexicon: !ref <data_folder>/lexicon_augmented.csv

# These three files are created from lexicon.csv.
csv_train: !ref <data_folder>/lexicon_train_subset.csv
csv_valid: !ref <data_folder>/lexicon_dev_subset.csv
csv_test: !ref <data_folder>/lexicon_test_subset.csv

# Training hyperparameters
N_epochs: 10
N_batch: 512
lr: 0.002
device: 'cuda:0'

# token infomation
bos: 40
eos: 40

train_loader: !new:speechbrain.data_io.data_io.DataLoaderFactory
    csv_file: !ref <csv_train>
    batch_size: !ref <N_batch>
    csv_read: [graphemes, phonemes]
    sentence_sorting: random
    output_folder: !ref <output_folder>
    replacements:
        $data_folder: !ref <data_folder>

valid_loader: !new:speechbrain.data_io.data_io.DataLoaderFactory
    csv_file: !ref <csv_valid>
    batch_size: !ref <N_batch>
    csv_read: [graphemes, phonemes]
    sentence_sorting: original
    output_folder: !ref <output_folder>
    replacements:
        $data_folder: !ref <data_folder>

test_loader: !new:speechbrain.data_io.data_io.DataLoaderFactory
    csv_file: !ref <csv_test>
    batch_size: !ref <N_batch>
    csv_read: [graphemes, phonemes]
    sentence_sorting: ascending
    output_folder: !ref <output_folder>
    replacements:
        $data_folder: !ref <data_folder>

oov_loader: !new:speechbrain.data_io.data_io.DataLoaderFactory
    csv_file: !ref <oov>
    batch_size: 2
    csv_read: [graphemes]
    sentence_sorting: original
    output_folder: !ref <output_folder>
    replacements:
        $data_folder: !ref <data_folder>

encoder_embed: !new:speechbrain.nnet.embedding.Embedding
    num_embeddings: 28  # 27 graphemes + 1 padding
    embedding_dim: 256

encoder_net: !new:speechbrain.nnet.RNN.LSTM
    bidirectional: True
    hidden_size: 256
    num_layers: 2

decoder_embed: !new:speechbrain.nnet.embedding.Embedding
    num_embeddings: 41  # 39 phonemes + 1 bos + 1 padding token
    embedding_dim: 256

decoder_net: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder
    rnn_type: gru
    attn_type: content
    hidden_size: 512
    attn_dim: 256
    dropout: 0.5
    num_layers: 3

decoder_linear: !new:speechbrain.nnet.linear.Linear
    n_neurons: 41  # 39 phonemes + 1 padding token (= 0) + 1 eos
    bias: True

logsoftmax: !new:speechbrain.nnet.activations.Softmax
    apply_log: True

compute_cost: !name:speechbrain.nnet.losses.nll_loss

optimizer: !new:speechbrain.nnet.optimizers.Adam_Optimizer
    learning_rate: !ref <lr>

lr_annealing: !new:speechbrain.nnet.lr_schedulers.NewBobLRScheduler
    improvement_threshold: 5
    annealing_factor: 1.0
    patient: 0

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>
    summary_fns:
        loss: !name:speechbrain.utils.train_logger.summarize_average
        PER: !name:speechbrain.utils.train_logger.summarize_error_rate

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <N_epochs>
